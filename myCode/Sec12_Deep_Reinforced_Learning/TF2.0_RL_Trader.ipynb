{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF2.0_RL_Trader.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wfG3UTcDYPFo","colab_type":"code","outputId":"93ad4e4a-5b8d-4e7c-bf21-469b4a020b0e","executionInfo":{"status":"ok","timestamp":1578681259371,"user_tz":-120,"elapsed":55398,"user":{"displayName":"Athanasios Chliopanos","photoUrl":"","userId":"16586829010922884016"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["# Install and import TF2\n","!pip install -q tensorflow==2.0.0\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 86.3MB 70kB/s \n","\u001b[K     |████████████████████████████████| 450kB 42.7MB/s \n","\u001b[K     |████████████████████████████████| 3.8MB 37.8MB/s \n","\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n","\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n","\u001b[?25h2.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bsyVaSXmYkHs","colab_type":"code","colab":{}},"source":["# More imports\n","import numpy as np\n","import pandas as pd\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense,Input\n","from tensorflow.keras.optimizers import Adam\n","\n","from datetime import datetime\n","import itertools\n","import argparse\n","import re\n","import os\n","import pickle\n","\n","from sklearn.preprocessing import StandardScaler"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w2viZw2leIpN","colab_type":"code","outputId":"136ff54d-f8c9-4b72-d8c3-881400e36e5a","executionInfo":{"status":"ok","timestamp":1578681437711,"user_tz":-120,"elapsed":2712,"user":{"displayName":"Athanasios Chliopanos","photoUrl":"","userId":"16586829010922884016"}},"colab":{"base_uri":"https://localhost:8080/","height":215}},"source":["# get the data\n","!wget https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/aapl_msi_sbux.csv"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-01-10 18:37:15--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/tf2.0/aapl_msi_sbux.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24098 (24K) [text/plain]\n","Saving to: ‘aapl_msi_sbux.csv’\n","\n","\raapl_msi_sbux.csv     0%[                    ]       0  --.-KB/s               \raapl_msi_sbux.csv   100%[===================>]  23.53K  --.-KB/s    in 0.007s  \n","\n","2020-01-10 18:37:16 (3.26 MB/s) - ‘aapl_msi_sbux.csv’ saved [24098/24098]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4UiKmz_ncpOx","colab_type":"code","colab":{}},"source":["# Let's use AAPL (Apple), MSI (Motorola), SBUX (Starbucks)\n","def get_data():\n","  # returns a T x 3 list of stock prices\n","  # each row is a different stock\n","  # 0 = AAPL\n","  # 1 = MSI\n","  # 2 = SBUX\n","  df = pd.read_csv('aapl_msi_sbux.csv')\n","  return df.values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLmJ2keveb6r","colab_type":"code","colab":{}},"source":["### The experience replay memory ###\n","class ReplayBuffer:\n","  def __init__(self,obs_dim,act_dim,size):\n","    self.obs1_buf = np.zeros([size, obs_dim], dtype=np.float32) # state\n","    self.obs2_buf = np.zeros([size, obs_dim], dtype=np.float32) # next state\n","    self.acts_buf = np.zeros(size, dtype=np.unit8) # actions\n","    self.rews_buf = np.zeros(size, dtype=np.float32)\n","    self.done_buf = np.zeros(size, dtype=np.unit8)\n","    self.ptr, self.size, self.max_size = 0, 0, size # buffer pointer\n","  \n","  # store transition into buffer\n","  def store(self, obs, act, rew, next_obs, done):\n","    self.obs1_buf[self.ptr] = obs\n","    self.obs2_buf[self.ptr] = next_obs\n","    self.acts_buf[self.ptr] = act\n","    self.rews_buf[self.ptr] = rew\n","    self.done_buf[self.ptr] = done\n","    self.ptr = (self.ptr+1) % self.max_size\n","    self.size = min(self.size+1, self.max_size)\n","\n","  # sampling by selecting random indices\n","  def sample_batch(self, batch_size=32):\n","    idxs = np.random.randint(0,self.size, size=batch_size)\n","    return dict(s=self.obs1_buf[idxs],\n","                s2=self.obs2_buf[idxs],\n","                a=self.acts_buf[idxs],\n","                r=self.rews_buf[idxs],\n","                d=self.done_buf[idxs])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uAp5kIxmHW0","colab_type":"code","colab":{}},"source":["def get_scaler(env):\n","  # return scikit-learn scaler object to scale the states\n","  # note: you could also populate the replay buffer here\n","\n","  states = []\n","  for _ in range(env.n_step):\n","    action = np.random.choice(env.action_space)\n","    state, reward, done, info = env.step(action)\n","    states.append(state)\n","    if done:\n","      break\n","  \n","  scaler = StandardScaler()\n","  scaler.fit(states)\n","  return scaler\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD0J1BgdsG_T","colab_type":"code","colab":{}},"source":["def maybe_make_dir(directory):\n","  if not os.path.exists(directory):\n","    os.makedirs(directory)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0BzIaPVsI_9","colab_type":"code","colab":{}},"source":["def mlp(input_dim, n_action, n_hidden_layers=1, hidden_dim=32):\n","  # A multi-layer-perceptron #\n","\n","  # input layer\n","  i = Input(shape=(input_dim,))\n","  x=i\n","\n","  # hidden layers\n","  for _ in range(n_hidden_layers):\n","    x = Dense(hidden_dim,activation='relu')(x)\n","  \n","  # final layer\n","  x = Dense(n_action)(x)\n","\n","  # make the model\n","  model = Model(i,x)\n","\n","  model.compile(loss='mse', optimizer='adam')\n","  print(model.summary())\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z3lKoZW7tWim","colab_type":"code","colab":{}},"source":["class MultiStockEnv:\n","  \"\"\"\n","    A 3-stock trading environment.\n","    State: vector of size 7 (n_stock * 2 + 1)\n","      - # shares of stock1 owned\n","      - # shares of stock2 owned\n","      - # shares of stock3 owned\n","      - price of stock 1 (using daily close price)\n","      - price of stock 2 (using daily close price)\n","      - price of stock 3 (using daily close price)\n","      - cash owned (can be used to purchase more stocks)\n","    Action: categorical variable with 27 (3^3) possibilities\n","      - for each stock you can:\n","        - 0 = sell\n","        - 1 = hold\n","        - 2 - buy\n","  \"\"\"\n","  ############# PUBLIC METHODS ################\n","\n","  # data input is a 2D array with a timeseries of stock prices\n","  def __init__(self, data, initial_investment=20000):\n","    # data\n","    self.stock_price_history = data\n","    self.n_step, self.n_stock = self.stock_price_history.shape\n","\n","    # instance attributes\n","    self.initial_investment = initial_investment\n","    self.cur_step = None\n","    self.stock_owned = None\n","    self.stock_price = None\n","    self.cash_in_hand = None\n","\n","    self.action_space = np.arange(3**self.n_stock)\n","\n","    # action permutations\n","    # returns a nested list with elements like:\n","    # [0,0,0]\n","    # [0,0,1]\n","    # [0,0,2]\n","    # ...\n","    # 0 = sell, 1 = hold, 2 = buy\n","    self.action_list = list(map(list, itertools.product([0,1,2],repeat=self.n_stock)))\n","\n","    # calculate size of state\n","    self.state_dim = self.n_stock * 2 + 1\n","\n","    self.reset()\n","\n","  def reset(self):\n","    self.cur_step = 0\n","    self.stock_owned = np.zeros(self.n_stock)\n","    self.stock_price = self.stock_price_history[self.cur_step]\n","    self.cash_in_hand = self.initial_investment\n","    return self._get_obs() # return state vector\n","\n","  def step(self,action):\n","    assert action in self.action_space\n","\n","    # get current value before performing the action\n","    prev_val = self._get_val()\n","\n","    # update price, i.e. go to the next day\n","    self.cur_step += 1\n","    self.stock_price = self.stock_price_history[self.cur_step]\n","\n","    # perform the trade\n","    self._trade(action)\n","\n","    # get the new value after taking te action\n","    cur_val = self._get_val()\n","\n","    # reward is the increase in portfolio value\n","    reward = cur_val - prev_val\n","\n","    # done if we have run out of data\n","    done = self.cur_step == self.n_step - 1\n","\n","    # store the current value of the portfolio here\n","    info = {'cur_val': cur_val}\n","\n","    # conform to the OpenAI Gym API\n","    return self._get_obs(), reward, done, info\n","\n","  ######## PRIVATE METHODS #############\n","\n","  def _get_obs(self):        # get state, a vector of 3 components\n","    obs = np.empty(self.state_dim)\n","    obs[:self.n_stock] = self.stock_owned # list of size 3\n","    obs[self.n_stock:2*self.n_stock] = self.stock_price # value of the stock\n","    obs[-1] = self.cash_in_hand\n","    return obs\n","  \n","  def _get_val(self):\n","    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n","  \n","  def _trade(self,action):\n","    # index the action we want to perform\n","    # 0 = sell\n","    # 1 = hold\n","    # 2 = buy\n","    # e.g. [2,1,0] means \n","    # buy 1st stock\n","    # hold 2nd stock\n","    # sell 3rd stock\n","    action_vec = self.action_list[action] # vector of size 3\n","\n","    # determine which stocks to buy or sell\n","    sell_index = [] # stores index of stocks we want to sell\n","    buy_index = [] # stores index of stocks we want to buy\n","    for i,a in enumerate(action_vec):\n","      if a == 0:\n","        sell_index.append(i)\n","      elif a == 2:\n","        buy_index.append(i)\n","    \n","    # sell any stocks we want to sell\n","    # then buy any stocks we want to buy\n","    if sell_index:\n","      # Note: to simplify the problem. when we sell, we sell ALL shares of that stock\n","      for i in sell_index:\n","        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n","        self.stock_owned[i] = 0\n","    if buy_index:\n","      # Note: when buying, we will loop through each stock we want to buy \n","      #       and buy one share at a time untiull we run out of cash\n","      can_buy = True\n","      while can_buy:\n","        for i in buy_index:\n","          if self.cash_in_hand > self.stock_price[i]:\n","            self.stock_owned[i] += 1 # buy one share\n","            self.cash_in_hand -= self.stock_price[i]\n","          else:\n","            can_buy = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1cMyAxQ9LYF","colab_type":"code","colab":{}},"source":["class DQNAgent(object):\n","  def __init__(self, state_size, action_size):\n","    self.state_size = state_size\n","    self.action_size = action_size\n","    self.memory = ReplayBuffer(state_size, action_size, size=500)\n","    self.gamma = 0.95 # discount rate\n","    self.epsilon = 1.0 # exploration rate\n","    self.epsilon_min = 0.01 \n","    self.epsilon_decay = 0.995\n","    self.model = mlp(state_size, action_size)\n","  \n","  def update_replay_memory(self, state, action, reward, next_state, done):\n","    self.memory.store(state, action, reward, next_state, done)\n","  \n","  def act(self, state):\n","    if np.random.rand() <= self.epsilon:\n","      return np.random.choice(self.action_size)\n","    act_values = self.model.predict(state)\n","    return np.argmax(act_values[0]) # return action\n","\n","  def replay(self, batch_size=32):\n","    # first check if replay buffer contains enough data\n","    if self.memory.size < batch_size:\n","      return\n","    \n","    # sample a batch of data from the replay memory\n","    minibatch = self.memory.sample_batch(batch_size)\n","    states = minibatch['s']\n","    actions = minibatch['a']\n","    rewards = minibatch['r']\n","    next_states = minibatch['s2']\n","    done = minibatch['d']\n","\n","    # Calculate the tentative target: Q(s',a)\n","    target = rewards + self.gamma * np.amax(self.model.predict(next_states),axis=1)\n","\n","    # The value of terminal states in zero\n","    # so set the target to be the reward only\n","    target[done] = rewards[done]\n","\n","    # target is 1D while predictions 2D (batch_size , num_of_actions) array\n","    # with the Keras API, the target (usually) must have the same\n","    # shape as the predictions\n","    # However, we only need to update the network for the actions\n","    # which were actually taken\n","    # we can accomplish this by setting the target to be equal to\n","    # the prediction for all values\n","    # then only change the targets for the actions taken\n","    # Q(s,a)\n","    target_full = self.model.predict(states)\n","    target_full[np.arange(batch_size), actions] = target # (row,col)\n","\n","    # Run one training step\n","    self.model.train_on_batch(states, target_full)\n","\n","    if self.epsilon > self.epsilon_min:\n","      self.epsilon *= self.epsilon_decay\n","\n","  def load(self, name):\n","    self.model.load_weights(name)\n","\n","  def save(self, name):\n","    self.model.save_weights(name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKIiMbzHKoGA","colab_type":"code","colab":{}},"source":["def play_one_episode(agent, env, is_train):\n","  # note: after transforming states are already 1xD\n","  state = env.reset()\n","  state = scaler.transform([state])\n","  done = False\n","\n","  while not done:\n","    action = agent.act(state)\n","    next_state, reward, done, info = env.step(action)\n","    next_state = scaler.transform([next_state])\n","    if is_train == 'train':\n","      agent.update_replay_memory(state, action, reward, next_state, done)\n","      agent.replay(batch_size)\n","    state = next_state\n"," \n","  return info('cur_val')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNk5wiYfK9bo","colab_type":"code","outputId":"8380919c-0ae0-4d5c-b6cd-5e42608b3230","executionInfo":{"status":"error","timestamp":1578681468461,"user_tz":-120,"elapsed":557,"user":{"displayName":"Athanasios Chliopanos","photoUrl":"","userId":"16586829010922884016"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["######### MAIN PROGRAM ##################\n","\n","if __name__ == '__main__':\n","\n","  # config\n","  models_folder = 'rl_trader_models'\n","  rewards_folder = 'rl_trader_rewards'\n","  num_episodes = 2000\n","  batch_size = 32\n","  initial_investment = 20000\n","\n","  parser = argparse.ArgumentParser()\n","  parser.add_argument('-m','--mode',type=str, required=True,\n","                      help='either \"train\" or \"test\"')\n","  args = parser.parse_args()\n","\n","  maybe_make_dir(models_folder)\n","  maybe_make_dir(rewards_folder)\n","\n","  data = get_data()\n","  n_timesteps,n_stocks = data.shape\n","\n","  n_train = n_timesteps // 2\n","\n","  train_data = data[:n_train]\n","  test_data = data[n_train:]\n","\n","  env = MultiStockEnv(train_data,initial_investment)\n","  state_size = env.state_dim\n","  action_size = len(env.action_space)\n","  agent = DQNAgent(state_size, action_size)\n","  scaler = get_scaler(env)\n","\n","  # store the final value of the portfolio (end of episode)\n","  portfolio_value = []\n","\n","  if args.mode == 'test':\n","    # then load the previous scaler\n","    with open(f'{models_folder}/scaler.pkl','rb') as f:\n","      scaler = pickle.load(f)\n","    \n","    # remake the env with test data\n","    env = MultiStockEnv(test_data,initial_investment)\n","\n","    # make sure epsilon is not 1!\n","    # no need to run multiple episodes if epsilon = 0, it's deterministic\n","    agent.epsilon = 0.01\n","\n","    # load trained weights\n","    agent.load(f'{models_folder}/dqn.h5')\n","  \n","  # play the game num_episodes times\n","  for e in range(num_episodes):\n","    t0 = datetime.now()\n","    val = play_one_episode(agent, env, args.mode)\n","    dt = datetime.now() - t0\n","    print(f\"episode: {e + 1}/{num_episodes}, episode and value: {val:.2f},duration: {dt}\")\n","    portfolio_value.append(val) # append episode end portfolio value\n","  \n","  # save the weights when we are done\n","  if args.mode == 'train':\n","    # save the DQN\n","    agent.save(f'{models_folder}/dqn.h5')\n","\n","    # save the scaler\n","    with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n","      pickle.dump(scaler,f)\n","  \n","  # save portfolio value  for each episode\n","  np.save(f'{rewards_folder}/{args.mode}.npy', portfolio_value)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] -m MODE\n","ipykernel_launcher.py: error: the following arguments are required: -m/--mode\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zPTe63k1ubx4","colab_type":"code","colab":{}},"source":["######### PLOT REWARDS  ###########\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('-m','--mode',type=str, required=True,\n","                      help='either \"train\" or \"test\"')\n","args = parser.parse_args()\n","\n","a = np.load(f'linear_rl_trader_rewards/{args.mode}.npy')\n","\n","print(f\"average reward: {a.mean():.2f}, min: {a.min():.2f}, max: {a.max():.2f}\")\n","\n","plt.hist(a, bins=20)\n","plt.title(args.mode)\n","plt.show()"],"execution_count":0,"outputs":[]}]}